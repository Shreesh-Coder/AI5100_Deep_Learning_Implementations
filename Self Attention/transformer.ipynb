{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads, similarity_fun=None):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_dim, out_dim)\n",
    "        self.key = nn.Linear(in_dim, out_dim)\n",
    "        self.value = nn.Linear(in_dim, out_dim)\n",
    "        self.heads = heads\n",
    "        self.out_dim_per_head = out_dim // heads\n",
    "        self.output = nn.Linear(out_dim, in_dim)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.query(x).view(batch_size, seq_len, self.heads, self.out_dim_per_head).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.heads, self.out_dim_per_head).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.heads, self.out_dim_per_head).transpose(1, 2)\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.out_dim_per_head, dtype=torch.float32))\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to the values\n",
    "        out = torch.matmul(attention, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        out = self.output(out)\n",
    "        \n",
    "        return self.gamma * out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_fun(Q: torch.tensor, K: torch.tensor):\n",
    "    # print(\"Q\", Q.shape)\n",
    "    # print(\"K.T\", K.transpose(1, 2).shape)\n",
    "    return torch.bmm(Q, K.transpose(1, 2))/torch.sqrt(torch.tensor(K.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input shape (batch_size, n_patches, C * patch_size * patch_size)\n",
    "class AddPositionEmbedding(nn.Module):\n",
    "    def __init__(self, num_patches, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))  # +1 for class token\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpLayer(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, dropout=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(mlp_dim, input_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, patch_size, att_dim, head, mlp_dim, num_patches, dropout=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_dim)\n",
    "        # Adjust the SelfAttentionLayer initialization if needed\n",
    "        self.att = SelfAttentionLayer(input_dim, att_dim, head, similarity_fun)\n",
    "        self.ln2 = nn.LayerNorm(att_dim)\n",
    "        self.mlp = MlpLayer(input_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first LayerNorm\n",
    "        x = self.ln1(x)\n",
    "        # Save the input for the skip connection\n",
    "        x_skip = x\n",
    "        # Apply the self-attention\n",
    "        x = self.att(x)\n",
    "        # Add the skip connection (residual)\n",
    "        x = x + x_skip\n",
    "\n",
    "        # Apply the second LayerNorm\n",
    "        x = self.ln2(x)\n",
    "        # Save the input for the second skip connection\n",
    "        x_skip = x\n",
    "        # Apply the MLP layer\n",
    "        x = self.mlp(x)\n",
    "        # Add the second skip connection (residual)\n",
    "        x = x + x_skip\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, num_channels, num_classes, emb_dim, num_heads, mlp_dim, num_layers, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.flatten_dim = patch_size * patch_size * num_channels\n",
    "        self.linear_proj = nn.Linear(self.flatten_dim, emb_dim)\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = AddPositionEmbedding(num_patches, emb_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            Encoder(input_dim=emb_dim, patch_size=patch_size, att_dim=emb_dim, head=num_heads, mlp_dim=mlp_dim, num_patches=num_patches, dropout=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unfold patches\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size) \\\n",
    "            .unfold(3, self.patch_size, self.patch_size)\n",
    "        # Reshape: [Batch, Channels, Patch_height, Patch_width, Num_patches_height, Num_patches_width]\n",
    "        x = x.permute(0, 4, 5, 1, 2, 3)\n",
    "        # Flatten patches\n",
    "        x = x.contiguous().view(x.size(0), -1, self.flatten_dim) # -1 here automatically calculates the correct number of patches\n",
    "        x = self.linear_proj(x)\n",
    "\n",
    "        # Add class token and position embeddings\n",
    "        class_tokens = self.class_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((class_tokens, x), dim=1)\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        # Process through the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Classifier token\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Assuming VisionTransformer class is defined as above\n",
    "\n",
    "# Define transformations and load CIFAR-10 dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize images to fit the model's expected input size\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),  # Apply random crops\n",
    "#     transforms.RandomHorizontalFlip(),  # Horizontal flipping\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Enhanced data transformations with Color Jitter and Random Erasing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)  # Random erasing\n",
    "])\n",
    "\n",
    "# For validation, we usually keep it simple\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./', train=False, download=True, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the model\n",
    "# model = VisionTransformer(img_size=224, patch_size=16, num_channels=3, num_classes=10, emb_dim=768, num_heads=12, mlp_dim=3072, num_layers=1, dropout_rate=0.5)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# Adjust model parameters for CIFAR-10\n",
    "model = VisionTransformer(\n",
    "    img_size=32,  # Adjusted image size\n",
    "    patch_size=4,  # Smaller patches to match the smaller image size\n",
    "    num_channels=3,\n",
    "    num_classes=10,\n",
    "    emb_dim=256,  # Reduced dimensionality\n",
    "    num_heads=8,  # Fewer heads to match the reduced dimensionality\n",
    "    mlp_dim=512,  # Smaller MLP size\n",
    "    num_layers=12,  # Keep a single layer for simplicity\n",
    "    dropout_rate=0.1  # Adjusted dropout rate\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Adjust the optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Introduce learning rate scheduling\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Loss function remains unchanged\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the device being used\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(state, filename=\"model_checkpoint.tar\"):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)  # Ensure directory exists\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler=None):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint.get('epoch', -1), checkpoint.get('best_accuracy', 0.0)\n",
    "\n",
    "def train_and_validate(model, train_loader, test_loader, optimizer, criterion, device, scheduler=None, num_epochs=5, checkpoint_path=None, filename=\"checkpoints/model_checkpoint.tar\"):\n",
    "    start_epoch = 0\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        start_epoch, best_accuracy = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
    "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch}), best accuracy: {best_accuracy}%\")\n",
    "        start_epoch += 1  # Continue from next epoch\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{start_epoch + num_epochs}', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs + start_epoch}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # Update the learning rate scheduler after each epoch\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc='Validating', leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        current_accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {current_accuracy:.2f}%')\n",
    "\n",
    "        # Save checkpoint if current accuracy is the best\n",
    "        if current_accuracy > best_accuracy:\n",
    "            print(\"Saving new best model\")\n",
    "            best_accuracy = current_accuracy\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_accuracy': best_accuracy\n",
    "            }\n",
    "            if scheduler is not None:\n",
    "                checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "            save_checkpoint(checkpoint, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.3327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 2.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 2.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 2.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 2.3042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 2.3041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 2.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/transformer/vit_layer_12_lr_0_001_op_adam_sch_Cosin.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_and_validate(model, train_loader, test_loader, optimizer, criterion, device, scheduler, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, filename\u001b[38;5;241m=\u001b[39mfilepath)\n",
      "Cell \u001b[1;32mIn[47], line 36\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, test_loader, optimizer, criterion, device, scheduler, num_epochs, checkpoint_path, filename)\u001b[0m\n\u001b[0;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 36\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Update the learning rate scheduler after each epoch\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath = \"checkpoints/transformer/vit_layer_12_lr_0_001_op_adam_sch_Cosin.tar\"\n",
    "train_and_validate(model, train_loader, test_loader, optimizer, criterion, device, scheduler, num_epochs=50, filename=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/transformer/vit_layer_1_lr_0_001_op_adam.tar' (epoch 37), best accuracy: 48.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/48], Loss: 1.4326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/48], Loss: 1.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/48], Loss: 1.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/48], Loss: 1.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.62%\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/48], Loss: 1.4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/48], Loss: 1.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/48], Loss: 1.4319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.95%\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/48], Loss: 1.4236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 49.00%\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/48], Loss: 1.4277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/48], Loss: 1.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_validate(model, train_loader, test_loader, optimizer, criterion, device, scheduler, num_epochs=10, checkpoint_path=filepath, filename=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
