{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = value.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = key.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Attention mechanism\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Example Usage\n",
    "embed_size = 256  # Size of the embedding vector\n",
    "heads = 8  # Number of attention heads\n",
    "\n",
    "self_attention_layer = SelfAttention(embed_size=embed_size, heads=heads)\n",
    "\n",
    "# Assuming a dummy input of shape (batch_size, sequence_length, embed_size)\n",
    "dummy_input = torch.rand((32, 64, embed_size))  # Example input tensor\n",
    "\n",
    "# Forward pass through the self-attention layer\n",
    "output = self_attention_layer(dummy_input, dummy_input, dummy_input)\n",
    "print(output.shape)  # Expected output shape: (batch_size, sequence_length, embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Assuming SelfAttention class is defined here (as provided in the previous response)\n",
    "\n",
    "class CNNWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(CNNWithAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.attention = SelfAttention(embed_size=512, heads=8)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(x.shape[0], -1, 512)  # Reshape for self-attention\n",
    "        x = self.attention(x, x, x)  # Self-attention\n",
    "        x = x.view(x.shape[0], -1)  # Flatten back\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='tiny-imagenet-200/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='tiny-imagenet-200/val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 2.10%\n",
      "Accuracy: 0.83%\n",
      "Accuracy: 1.32%\n",
      "Accuracy: 0.49%\n",
      "Accuracy: 0.42%\n",
      "Accuracy: 0.94%\n",
      "Accuracy: 0.25%\n",
      "Accuracy: 0.54%\n",
      "Accuracy: 0.59%\n",
      "Accuracy: 0.59%\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNWithAttention(num_classes=200).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Num of epochs\n",
    "    model.train()\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(f'Accuracy: {float(num_correct)/float(num_samples)*100:.2f}%')\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
